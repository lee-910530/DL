#%%
import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import yaml
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import random

# 設置CUDA_LAUNCH_BLOCKING
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# 定義變換
transform = transforms.Compose([
    transforms.Resize((200, 200)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

class TrafficDataset(Dataset):
    def __init__(self, images_dir, labels_dir, transform=None):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.image_files = os.listdir(images_dir)
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_file = self.image_files[idx]
        image_path = os.path.join(self.images_dir, image_file)
        image = Image.open(image_path).convert("RGB")
        width, height = image.size
        
        label_file = os.path.splitext(image_file)[0] + ".txt"
        label_path = os.path.join(self.labels_dir, label_file)
        
        labels = []
        with open(label_path, "r") as f:
            lines = f.read().strip().split("\n")
            for line in lines:
                parts = line.split()
                if len(parts) != 5:
                    continue
                class_id, x_center, y_center, width_ratio, height_ratio = map(float, parts)
                x_center *= width
                y_center *= height
                box_width = width_ratio * width
                box_height = height_ratio * height
                x_min = x_center - box_width / 2
                y_min = y_center - box_height / 2
                x_max = x_center + box_width / 2
                y_max = y_center + box_height / 2
                if (x_max - x_min) > 0 and (y_max - y_min) > 0:
                    labels.append([class_id, x_min, y_min, x_max, y_max])
        
        if self.transform:
            # Apply the transform to the image
            transformed_image = self.transform(image)
            # Calculate the scaling factors
            new_width, new_height = transformed_image.shape[2], transformed_image.shape[1]
            scale_x = new_width / width
            scale_y = new_height / height
            # Apply the scaling to the bounding boxes
            for label in labels:
                label[1] *= scale_x
                label[2] *= scale_y
                label[3] *= scale_x
                label[4] *= scale_y
        
        if len(labels) > 0:
            labels = torch.tensor(labels)
        else:
            labels = torch.empty((0, 5))
        
        return transformed_image, labels

def collate_fn(batch):
    return tuple(zip(*batch))

def get_model(num_classes):
    # 加載預訓練的Faster R-CNN模型
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    # 獲取模型的分類器輸入特徵數
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # 替換頭部的分類器
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

def train_one_epoch(model, optimizer, train_loader, device, epoch):
    model.train()
    total_train_loss = 0
    train_progress_bar = tqdm(train_loader, desc=f"Epoch {epoch} - Training")
    
    for images, targets in train_progress_bar:
        images = list(image.to(device) for image in images)
        targets = [{ 'boxes': target[:, 1:].to(device), 'labels': target[:, 0].long().to(device) } for target in targets]
        
        # Forward pass
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())  # Summing up all losses
        
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        
        total_train_loss += losses.item()
        train_progress_bar.set_postfix(train_loss=total_train_loss / len(train_loader))
    
    avg_train_loss = total_train_loss / len(train_loader)
    print(f"Epoch: {epoch}, Train Loss: {avg_train_loss}")
    
    return avg_train_loss

def validate_model(model, val_loader, device, epoch, num_images_to_show=5):
    model.eval()
    with torch.no_grad():
        images_list = []
        predictions_list = []
        
        for images, targets in tqdm(val_loader, desc="Validation"):
            images = list(img.to(device) for img in images)
            targets = [{ 'boxes': target[:, 1:].to(device), 'labels': target[:, 0].long().to(device) } for target in targets]
            
            # Forward pass
            predictions = model(images)
            
            images_list.extend(images)
            predictions_list.extend(predictions)
        
        # Randomly select images to visualize
        num_images = min(len(images_list), num_images_to_show)
        selected_indices = random.sample(range(len(images_list)), num_images)
        
        # Denormalize and visualize predictions
        fig, axs = plt.subplots(1, num_images, figsize=(20, 5))
        for i, idx in enumerate(selected_indices):
            image = images_list[idx]
            prediction = predictions_list[idx]
            
            mean = torch.tensor([0.5, 0.5, 0.5]).to(device)
            std = torch.tensor([0.5, 0.5, 0.5]).to(device)
            image = image * std[:, None, None] + mean[:, None, None]
            image = image.cpu().clamp(0, 1)

            img = transforms.ToPILImage()(image)
            ax = axs[i]
            ax.imshow(img)
            
            boxes = prediction['boxes'].cpu().numpy()
            labels = prediction['labels'].cpu().numpy()
            
            for box, label in zip(boxes, labels):
                box = [int(b) for b in box]
                ax.add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, edgecolor='red', lw=2))
                ax.text(box[0], box[1], f'Class {label}', bbox=dict(facecolor='white', alpha=0.5))
            
            ax.axis('off')
        
        plt.show()
        torch.save(model.state_dict(), f"model_epoch_{epoch}.pt")

def main():
    # Load parameters from YAML file
    with open('./trafic_data/data_1.yaml', 'r', encoding='utf-8') as f:
        prmtr = yaml.load(f, Loader=yaml.FullLoader)
        train_images = prmtr['train_images']
        train_labels = prmtr['train_labels']
        val_images = prmtr['val_images']
        val_labels = prmtr['val_labels']
        batch_size = prmtr['batch_sizes']
        num_workers = prmtr['num_workers']
        num_classes = prmtr['num_classes']
        num_epochs = prmtr['num_epochs']

    # Create training dataset and data loader
    train_dataset = TrafficDataset(train_images, train_labels, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)

    val_dataset = TrafficDataset(val_images, val_labels, transform=transform)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)

    # Get device
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(device)

    # Initialize model
    model = get_model(num_classes)
    model.to(device)

    # Set optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Training
    train_losses = []
    for epoch in range(num_epochs):
        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
        train_losses.append(train_loss)
        
        # Validate and visualize results
        validate_model(model, val_loader, device, epoch)
    
    # Optionally plot training losses
    plt.figure()
    plt.plot(train_losses, label='Train Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()

# %%
